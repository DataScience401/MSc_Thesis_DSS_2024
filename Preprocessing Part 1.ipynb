{"cells":[{"cell_type":"markdown","metadata":{"id":"xCmdxamO7NPp"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sWehyBiE7NPr"},"outputs":[],"source":["# Standard library imports\n","from datetime import datetime\n","\n","# Data handling and numerical processing\n","import pandas as pd\n","import numpy as np\n","\n","# Geographic data handling\n","import geopandas as gpd\n","import folium\n","from folium.plugins import HeatMap\n","\n","# Data transformation and preprocessing\n","from sklearn.preprocessing import (\n","    OneHotEncoder,\n","    OrdinalEncoder,\n","    StandardScaler,\n","    LabelEncoder,\n","    MultiLabelBinarizer\n",")\n","from pyproj import Proj, transform\n","\n","# Model selection and resampling\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import SMOTENC\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import LogNorm\n"]},{"cell_type":"markdown","metadata":{"id":"7I7BsUZw7NPr"},"source":["## Loading Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAyFHDYD7NPs"},"outputs":[],"source":["#Load data\n","party_raw_a= pd.read_csv('Data raw/Ongevallengegevens/partijen.txt',dtype=str)\n","rta_raw_a= pd.read_csv('Data raw/Ongevallengegevens/ongevallen.txt',dtype=str)\n","pointlocations= pd.read_csv('Data raw/Netwerkgegevens/puntlocaties.txt')\n","weather_raw = pd.read_csv('Data raw/Weather/De Bilt.txt',dtype=str)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Dz-UzSO7NPs"},"outputs":[],"source":["#Load later transfered data\n","rta_raw_b=pd.read_csv('Data raw/Data transfer Rijkswaterstaat/Ongevallen.txt',delimiter=';',dtype=str)\n","party_raw_b=pd.read_csv('Data raw/Data transfer Rijkswaterstaat/Partijen.txt',delimiter=';',dtype=str)\n","partyextra_b=pd.read_csv('Data raw/Data transfer Rijkswaterstaat/Partijaanvullingen.txt',delimiter=';')\n","victims_b=pd.read_csv('Data raw/Data transfer Rijkswaterstaat/Slachtoffers.txt',delimiter=';',dtype=str)"]},{"cell_type":"markdown","metadata":{"id":"4EJIyoOw7NPs"},"source":["## Preprocessing RTA Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-d3ePdNy7NPs"},"outputs":[],"source":["columns_to_drop = ['DATUM_VKL', 'DAG_CODE', 'MND_NUMMER', 'JAAR_VKL', 'TIJDSTIP',\n","                   'UUR', 'DDL_ID', 'AP4_CODE','AP5_CODE',\n","                   'ANTL_SLA', 'ANTL_DOD', 'ANTL_GZH', 'ANTL_SEH', 'ANTL_GOV',\n","                   'ANTL_PTJ', 'ANTL_TDT', 'MNE_CODE','MND_NUMMER']\n","\n","# Drop the specified columns inplace\n","rta_raw_a.drop(columns=columns_to_drop, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BLfLeWd7NPt"},"outputs":[],"source":["# Define the colorblind-friendly CUD palette with additional colors\n","palette = ['#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7', '#999999', '#F8A19F', '#6A3D9A',\n","           '#E6A3DC', '#B3DE3F', '#F2BC40', '#99D7E3', '#BB47B8', '#FBE64B', '#8B8B3A', '#E5AB97', '#43D0B2', '#FF69B4',\n","           '#8A2BE2', '#DEB887']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbJtPp087NPt"},"outputs":[],"source":["rta_raw_b['VKL_NUMMER'] = rta_raw_b['VKL_NUMMER'].str.split(',').str[0]\n","rta_raw_a['VKL_NUMMER'] = rta_raw_a['VKL_NUMMER'].str.split(',').str[0]\n","\n","# Convert 'VKL_NUMMER' column to integer in both dataframes\n","rta_raw_b['VKL_NUMMER'] = rta_raw_b['VKL_NUMMER'].astype(int)\n","rta_raw_a['VKL_NUMMER'] = rta_raw_a['VKL_NUMMER'].astype(int)\n","\n","# Perform inner merge on 'VKL_NUMMER' column\n","rta_raw = pd.merge(rta_raw_b, rta_raw_a, on='VKL_NUMMER', how='inner')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3Vdw3Mu7NPt"},"outputs":[],"source":["#Maybe not drop\n","rta_filtered=rta_raw\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wj5Ta3X07NPt"},"outputs":[],"source":["# Convert DATUM_VKL to datetime format\n","rta_filtered['DATUM_VKL'] = pd.to_datetime(rta_filtered['DATUM_VKL'], format='%Y%m%d')\n","\n","# Extract year, month, and weekday\n","rta_filtered['YEAR'] = rta_filtered['DATUM_VKL'].dt.year\n","rta_filtered['MONTH'] = rta_filtered['DATUM_VKL'].dt.month\n","rta_filtered['WEEKDAY'] = rta_filtered['DATUM_VKL'].dt.weekday\n","\n","# Create a new column indicating weekend (1 for weekend, 0 for weekday)\n","rta_filtered['WEEKEND'] = rta_filtered['WEEKDAY'].apply(lambda x: 1 if x >= 5 else 0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"csz6IQIY7NPt"},"outputs":[],"source":["def concatenate_columns(df, col_list, new_col_name):\n","    # Replace empty strings with NaN\n","    df[col_list] = df[col_list].replace('', np.nan)\n","\n","    # Concatenate the columns, replace NaN values with an empty string and join the non-empty values with a semicolon\n","    df[new_col_name] = df[col_list].apply(lambda row: ';'.join(row.dropna()), axis=1)\n","\n","# Define the columns to concatenate for BZD_VM\n","columns_to_concat = ['BZD_ID_VM1', 'BZD_ID_VM2', 'BZD_ID_VM3']\n","concatenate_columns(rta_filtered, columns_to_concat, 'BZD_VM')\n","\n","# Repeat the same process for BZD_IF\n","columns_to_concat = ['BZD_ID_IF1', 'BZD_ID_IF2', 'BZD_ID_IF3']\n","concatenate_columns(rta_filtered, columns_to_concat, 'BZD_IF')\n","\n","# Repeat the same process for BZD_TA\n","columns_to_concat = ['BZD_ID_TA1', 'BZD_ID_TA2', 'BZD_ID_TA3']\n","concatenate_columns(rta_filtered, columns_to_concat, 'BZD_TA')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-mG6C337NPt"},"outputs":[],"source":["palette = {\n","    \"dark_blue\": \"#003366\",\n","    \"orange_brown\": \"#cc9933\",\n","    \"green\": \"#339900\",\n","}\n","\n","highlight_color = \"#008EC6\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LYjegqX7NPu"},"outputs":[],"source":["category_mapping = {'DOD': 1, 'LZW': 1, 'LLI': 0, 'UMS': 0}\n","rta_filtered['SEVERE'] = rta_filtered['AP4_CODE'].map(category_mapping)\n","\n","# Group by AP4_CODE and SEVERE and count occurrences\n","code_severity_counts = rta_filtered.groupby(['AP4_CODE', 'SEVERE']).size().unstack(fill_value=0)\n","\n","# Sorting the DataFrame according to a specific order\n","order = ['UMS', 'LLI', 'LZW', 'DOD']\n","code_severity_counts = code_severity_counts.reindex(order)\n","\n","# Define colors for the severity levels\n","palette = {\"dark_blue\": \"#003366\", \"orange_brown\": \"#cc9933\", \"green\": \"#339900\"}\n","colors = [palette['dark_blue'], palette['orange_brown']]  # Using dark blue for non-severe, orange brown for severe\n","\n","# Plotting configuration\n","code_severity_counts.plot(kind='bar', stacked=True, color=colors)\n","plt.title('Counts of AP4_CODE by Severity')\n","plt.xlabel('AP4_CODE')\n","plt.ylabel('Counts')\n","plt.xticks(rotation=0)  # Keep labels horizontal\n","plt.legend(title='Severity', labels=['Non-severe', 'Severe'])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mAGTnYig7NPu"},"outputs":[],"source":["# Define the colorblind-friendly CUD palette with additional colors\n","palette = ['#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7', '#999999', '#F8A19F', '#6A3D9A',\n","           '#E6A3DC', '#B3DE3F', '#F2BC40', '#99D7E3', '#BB47B8', '#FBE64B', '#8B8B3A', '#E5AB97', '#43D0B2', '#FF69B4',\n","           '#8A2BE2', '#DEB887']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aI3gOiNA7NPu"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"G9oVu4Vw7NPu"},"source":["## for the data description use the final dataset and the orignal, plot them here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7yhCv5u7NPu"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnaHqnaZ7NPu"},"outputs":[],"source":["remove_columns=['BZD_ID_VM1', 'BZD_ID_VM2', 'BZD_ID_VM3', 'BZD_VM_AN',\n","                      'BZD_ID_IF1','BZD_ID_IF2', 'BZD_ID_IF3', 'BZD_IF_AN',\n","                      'BZD_ID_TA1', 'BZD_ID_TA2','BZD_ID_TA3', 'BZD_TA_AN',\n","                      'WDK_AN','WSE_AN','WVG_AN','HUISNUMMER',\n","                      'GME_NAAM', 'PVE_NAAM', 'KDD_NAAM', 'PLT_NAAM',\n","                      'DIENSTCODE', 'DIENSTNAAM', 'DISTRCODE', 'DISTRNAAM',\n","                      'DAGTYPE','WEEKNR','REGNUMMER', 'PVOPGEM', 'AP5_CODE',\n","                      'ZAD_ID','WGD_CODE_2','HECTOMETER','JTE_ID','WVK_ID','AP3_CODE',\n","                      'AP4_CODE', 'ANTL_DOD','ANTL_SLA','ANTL_GOV', 'ANTL_GZH',\n","                      'ANTL_SEH','NIVEAUKOP']\n","# Calculating the missing value percentage for each column before dropping them\n","missing_percentage_before_drop = rta_filtered[remove_columns].isnull().mean() * 100\n","\n","missing_percentage_before_drop = missing_percentage_before_drop.to_frame(name='Missing Value Percentage')\n","missing_percentage_before_drop.reset_index(inplace=True)\n","missing_percentage_before_drop.columns = ['Column', 'Missing Value Percentage']\n","\n","\n","print(missing_percentage_before_drop)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"okvvw0227NPu"},"outputs":[],"source":["rta_filtered['ANTL_DOD'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Y5JcBfe7NPu"},"outputs":[],"source":["rta_filtered.drop(columns=remove_columns,inplace=True)\n","\n","rta=rta_filtered"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8uhSKdNp7NPu"},"outputs":[],"source":["# Add coordinates to rta dataset\n","rta=pd.merge(rta_filtered,pointlocations,on='FK_VELD5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THO43hIF7NPu"},"outputs":[],"source":["rta_raw['GME_ID'].nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x6ZExff77NPu"},"outputs":[],"source":["\n","from pyproj import CRS, Transformer\n","\n","# Function to convert RD to GPS\n","def rd_to_gps(x, y):\n","    # Define the coordinate systems using CRS objects\n","    rd_crs = CRS('EPSG:28992')  # RD New\n","    wgs84_crs = CRS('EPSG:4326')  # WGS 84\n","\n","    # Create a Transformer object to perform the conversion\n","    transformer = Transformer.from_crs(rd_crs, wgs84_crs, always_xy=True)\n","\n","    # Perform the transformation\n","    lon, lat = transformer.transform(x, y)\n","    return lat, lon\n","\n","\n","# Apply the conversion function\n","rta['latitude'], rta['longitude'] = zip(*rta.apply(lambda row: rd_to_gps(row['X_COORD'], row['Y_COORD']), axis=1))\n","\n","# Create a new dataframe with the selected columns\n","coordinates = rta[['VKL_NUMMER', 'SEVERE', 'latitude', 'longitude']]\n","\n","# Save the result to a new CSV file\n","coordinates.to_csv('converted_coordinates.csv', index=False)\n","\n","print(\"Conversion and saving complete!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Hcl_l097NPu"},"outputs":[],"source":["coordinates = pd.read_csv('converted_coordinates.csv')\n","coordinates.drop('SEVERE',axis=1,inplace=True)\n","rtacoord=pd.merge(rta,coordinates,on='VKL_NUMMER')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BlQEJjnl7NPu"},"outputs":[],"source":["coordinates.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gigEikkC7NPu"},"outputs":[],"source":["rta= rtacoord"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tqNeawco7NPu"},"outputs":[],"source":["rta.head()"]},{"cell_type":"markdown","metadata":{"id":"VOh-WRT67NPu"},"source":["## SEVERE Only"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2F1UjJLM7NPu"},"outputs":[],"source":["\n","# Filter data to include only coordinates within the boundaries of the Netherlands\n","netherlands = df[(df['latitude'] >= 50) & (df['latitude'] <= 54) & (df['longitude'] >= 3) & (df['longitude'] <= 8)]\n","\n","# Map severity values\n","severity_map = {0: 'Non-Severe', 1: 'Severe'}\n","netherlands['SEVERITY_LABEL'] = netherlands['SEVERE'].map(severity_map)\n","\n","# Get unique categories\n","severities = netherlands['SEVERITY_LABEL'].unique()\n","types = netherlands['TYPE'].unique()\n","\n","# Create subplots\n","fig, axes = plt.subplots(len(severities), len(types), figsize=(15, 10), sharex=True, sharey=True)\n","\n","for i, severity in enumerate(severities):\n","    for j, type_ in enumerate(types):\n","        subset = netherlands[(netherlands['SEVERITY_LABEL'] == severity) & (netherlands['TYPE'] == type_)]\n","        if subset.empty:\n","            axes[i, j].set_visible(False)\n","        else:\n","            hb = axes[i, j].hexbin(subset['longitude'], subset['latitude'], gridsize=250, cmap='viridis', norm=LogNorm())\n","            axes[i, j].set_title(f'{type_},{severity}', fontsize=22)\n","            if i == len(severities) - 1:\n","                axes[i, j].set_xlabel('Longitude', fontsize=20)\n","            if j == 0:\n","                axes[i, j].set_ylabel('Latitude', fontsize=20)\n","            cbar = fig.colorbar(hb, ax=axes[i, j])\n","            cbar.ax.tick_params(labelsize=16)\n","\n","# Adjust font sizes for tick labels\n","for ax in axes.flat:\n","    if ax.get_visible():\n","        ax.tick_params(axis='both', which='major', labelsize=16)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qcfh-gTN7NPv"},"outputs":[],"source":["rta.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXvDI4Xq7NPv"},"outputs":[],"source":["#Lastig de verkeersintensiteit mee te pakken\n","# Ik neem coronajaren mee als feture, want ik denk niet dat covid wel impact heeft op de hoveelheid ongelukken maar niet op de severity. Afweging meer data prioriteit\n","# After the outbreak of COVID-19, aggressiveness is estimated to increase by 0.023 and inattentiveness is estimated to increase by 0.015, resulting in a total indirect effect of 0.205 (3.598 × 0.023 + 8.123 × 0.015) on severity propensity. However, according to the probit model in Table 4, COVID-19 is not found to affect crash severity significantly, likely because risky driving behaviors (e.g., speeding, distraction) that COVID-19 is associated with are used to model crash severity directly.\n","# Unlike the SEM assumption that COVID-19 affected crash severity via changing driving behaviors, the probit model assumes COVID-19 imposed a direct effect on crash severity. Estimation results of the probit model are reported in Table 4 . All the explanatory variables were regarded as statistically significant at 95% level (p-values < 0.05) in the proposed SEM, whereas the variables COVID-19 and distraction were found to be insignificant in the probit model for the crash severity.\n","# heeft impact, maar wellicht niet al te significant en wordt met jaar al meegepakt.\n","#imapct of covid on crash severity. Heeft wel impact, maar de mate verschillend. Binary zou niet jjustice doen en data is schaars dus ik doe het gewoon per jaar, na covid is de geiddelde snelheid vgm ook hoger\n","# How did COVID-19 impact driving behaviors and crash Severity? A multigroup structural equation modeling\n","# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9042805/\n","\n","# Meer alcohol en drugs cases o.a. behaviour is dus anders, maar ook na covid. Ik dnek dat jaar dit het beste kan oppakken, eerder dan een binary variable\n","# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9874053/\n","rta['DATUM_VKL'] = pd.to_datetime(rta['DATUM_VKL'])\n","\n","# Define the start and end dates\n","#start_date = pd.to_datetime('2020-03-01')\n","#end_date = pd.to_datetime('2021-05-01')\n","\n","\n","# Filter out rows falling within the specified date range\n","\n","\n","# Convert 'DATUM_VKL' column back to the original format\n","rta['DATUM_VKL'] = rta['DATUM_VKL'].dt.strftime('%Y%m%d')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sHOVQsQG7NPv"},"outputs":[],"source":["print(rta.shape)\n","print(rta['SEVERE'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4GWcdJjM7NPv"},"outputs":[],"source":["rta.to_csv('Processed data/rta.csv',index=True)"]},{"cell_type":"markdown","metadata":{"id":"REKiy0sG7NPv"},"source":["## Weather\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_B1i056R7NPv"},"outputs":[],"source":["weather_raw['YYYYMMDD']=weather_raw['YYYYMMDD'].astype(int)\n","\n","# Filter the datetime between 2013 and 2022\n","weather_filtered = weather_raw[(weather_raw['YYYYMMDD'] >= 20130101) & (weather_raw['YYYYMMDD'] <= 20221231)]\n","\n","# Remove leading spaces from column names\n","weather_filtered.columns = weather_filtered.columns.str.strip()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GyvUTSU7NPv"},"outputs":[],"source":["numeric_columns = ['FG', 'FHX', 'FHN', 'FXX', 'TG', 'TN', 'TX', 'SQ', 'DR', 'RH', 'PG', 'VVN', 'VVX', 'NG']\n","\n","# Convert columns to numeric safely using .loc to avoid SettingWithCopyWarning\n","for column in numeric_columns:\n","    weather_filtered.loc[:, column] = pd.to_numeric(weather_filtered.loc[:, column], errors='coerce')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7Jqvys07NPv"},"outputs":[],"source":["# Handling special cases for precipitation where -1 indicates values below measurable limits --> 0.01 to show there is something rather then nothing, but very small\n","weather_filtered = weather_filtered.copy()  # Create a copy of the DataFrame\n","weather_filtered['RH'] = weather_filtered['RH'].apply(lambda x: 0.01 if x == -1 else x)\n","weather_filtered['RHX'] = weather_filtered['RHX'].apply(lambda x: 0.01 if x == -1 else x)\n","\n","# Selecting specific columns and making a copy for further processing\n","weather_processed = weather_filtered[['YYYYMMDD', 'FG', 'FHX', 'FHN', 'FXX', 'TG', 'TN', 'TX', 'SQ', 'DR', 'RH', 'PG', 'VVN', 'VVX', 'NG']].copy()\n","weather_processed.columns = [\n","    'DATUM_VKL',\n","    'MeanWindSpeed_mps',\n","    'MaxHourlyWindSpeed_mps',\n","    'MinHourlyWindSpeed_mps',\n","    'MaxWindGust_mps',\n","    'MeanTemperature_C',\n","    'MinTemperature_C',\n","    'MaxTemperature_C',\n","    'SunshineDuration_hrs',\n","    'PrecipitationDuration_hrs',\n","    'TotalDailyPrecip_mm',\n","    'MeanSLPressure_hPa',\n","    'MinVisibility_km',\n","    'MaxVisibility_km',\n","    'MeanCloudCover_oct'\n","]\n","\n","# Define columns to convert and apply conversion factor\n","cols_to_convert = [\n","    'MeanWindSpeed_mps', 'MaxHourlyWindSpeed_mps', 'MinHourlyWindSpeed_mps',\n","    'MaxWindGust_mps', 'MeanTemperature_C', 'MinTemperature_C',\n","    'MaxTemperature_C', 'SunshineDuration_hrs', 'PrecipitationDuration_hrs',\n","    'TotalDailyPrecip_mm', 'MeanSLPressure_hPa'\n","]\n","\n","# Convert selected columns to numeric types, apply conversion factor, and handle errors\n","for col in cols_to_convert:\n","    weather_processed[col] = pd.to_numeric(weather_processed[col], errors='coerce') * 0.1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UW7XMIjJ7NPv"},"outputs":[],"source":["weather_processed['DATUM_VKL']=weather_processed['DATUM_VKL'].astype(str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtWxos7h7NPz"},"outputs":[],"source":["rta= pd.merge(rta,weather_processed,on='DATUM_VKL',how='left')"]},{"cell_type":"markdown","metadata":{"id":"QBn48m_47NPz"},"source":["## Parties involved CHECKPOINT!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdYL7aqn7NPz"},"outputs":[],"source":["\n","party_merged= pd.merge(party_raw_a,party_raw_b,on='PTJ_ID')\n","party_merged = party_merged.drop(columns=[col for col in party_merged.columns if col.endswith('_x')])\n","party_merged.columns = [col.replace('_y', '') if '_y' in col else col for col in party_merged.columns]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VSBTHQsz7NPz"},"outputs":[],"source":["empty_cols = [col for col in party_merged.columns if party_merged[col].isna().all()]\n","party_merged.drop(columns=empty_cols, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGFG_6XB7NPz"},"outputs":[],"source":["party_merged.columns"]},{"cell_type":"markdown","metadata":{"id":"NonoqrEM7NPz"},"source":["Meaning that LIKE_ID and Geslacht are often missing together, but not too often with objectype, so  age could be inputed  by mode of objet type. Unfortunately not that many other columns to use for imputing or using KNN, as most columns are almsot empty.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-PnoGon7NPz"},"outputs":[],"source":["party_merged['VKL_NUMMER'] = party_merged['VKL_NUMMER'].str.split(',').str[0].astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7cio2q-T7NPz"},"outputs":[],"source":["party_merged = party_merged.merge(rta[['VKL_NUMMER', 'SEVERE']], on='VKL_NUMMER', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DO6ejy7g7NPz"},"outputs":[],"source":["party_merged.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"COqW9dnM7NPz"},"outputs":[],"source":["import seaborn as sns\n","from matplotlib.colors import LogNorm\n","\n","# Example data processing setup\n","# Calculate NaN percentages for each VKL_NUMMER for each 'SEVERE' category\n","nan_percentages = party_merged.groupby(['VKL_NUMMER', 'SEVERE']).apply(\n","    lambda df: pd.Series({\n","        'OTE_ID_NaN_Percentage': df['OTE_ID'].isna().mean() * 100,\n","        'LKE_ID_NaN_Percentage': df['LKE_ID'].isna().mean() * 100\n","    })).reset_index()\n","\n","# Binning the NaN percentages into 10 equal bins\n","nan_percentages['OTE_ID_Bin'] = pd.cut(nan_percentages['OTE_ID_NaN_Percentage'], bins=10, labels=False, include_lowest=True)\n","nan_percentages['LKE_ID_Bin'] = pd.cut(nan_percentages['LKE_ID_NaN_Percentage'], bins=10, labels=False, include_lowest=True)\n","\n","# Create separate pivot tables for SEVERE=0 and SEVERE=1\n","heatmap_data_severe_0 = nan_percentages[nan_percentages['SEVERE'] == 0].pivot_table(\n","    index='LKE_ID_Bin', columns='OTE_ID_Bin', values='VKL_NUMMER', aggfunc='count', fill_value=0)\n","\n","heatmap_data_severe_1 = nan_percentages[nan_percentages['SEVERE'] == 1].pivot_table(\n","    index='LKE_ID_Bin', columns='OTE_ID_Bin', values='VKL_NUMMER', aggfunc='count', fill_value=0)\n","\n","# Using subplots to plot the heatmaps\n","fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 8))\n","\n","# Heatmap for SEVERE=0\n","sns.heatmap(heatmap_data_severe_0, annot=True, fmt=\"d\", cmap='Blues', norm=LogNorm(vmin=1, vmax=heatmap_data_severe_0.max().max()), ax=axes[0])\n","axes[0].set_title('Heatmap of VKL_NUMMER Count by NaN Percentage Bins for SEVERE=0')\n","axes[0].set_xlabel('OTE_ID NaN Percentage Bins')\n","axes[0].set_ylabel('LKE_ID NaN Percentage Bins')\n","axes[0].invert_yaxis()  # Invert the y-axis\n","axes[0].tick_params(axis='y', which='both', labelleft=True)  # Ensure y-axis labels are visible\n","axes[0].get_xaxis().set_visible(False)  # Hide x-axis labels\n","\n","# Heatmap for SEVERE=1\n","sns.heatmap(heatmap_data_severe_1, annot=True, fmt=\"d\", cmap='Blues', norm=LogNorm(vmin=1, vmax=heatmap_data_severe_1.max().max()), ax=axes[1])\n","axes[1].set_title('Heatmap of VKL_NUMMER Count by NaN Percentage Bins for SEVERE=1')\n","axes[1].set_xlabel('OTE_ID NaN Percentage Bins')\n","axes[1].set_ylabel('')  # Remove y-axis label\n","axes[1].invert_yaxis()  # Invert the y-axis\n","axes[1].tick_params(axis='y', which='both', labelleft=True)  # Ensure y-axis labels are visible\n","axes[1].get_xaxis().set_visible(False)  # Hide x-axis labels\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bgRfP20H7NPz"},"outputs":[],"source":["nan_counts = party_merged.isna().sum()\n","print(nan_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cm3Gz1zZ7NPz"},"outputs":[],"source":["# Counting non-NaN values in each column\n","non_nan_counts = party_merged.count()\n","\n","# Displaying the counts\n","print(non_nan_counts)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aYL82u1t7NPz"},"outputs":[],"source":["# Grouping the data and creating the matrix\n","matrix = party_merged.groupby(['OTE_ID', 'LKE_ID'], dropna=False).size().unstack(fill_value=0)\n","\n","# Setting up the matplotlib figure\n","plt.figure(figsize=(12, 10))\n","\n","# Defining the logarithmic norm. We add 1 to the matrix values to handle zero values because log(0) is undefined.\n","log_norm = LogNorm(vmin=1, vmax=matrix.max().max())\n","\n","# Choosing a colormap that provides good contrast in the log scale\n","color_scale = sns.diverging_palette(220, 20, as_cmap=True)\n","\n","# Drawing the heatmap\n","ax = sns.heatmap(matrix, annot=True, fmt=\"d\", cmap='Blues', norm=log_norm, cbar_kws={'label': 'Log-scaled Count of Records'})\n","\n","# Adding title and formatting\n","plt.title('Logarithmic Scaled Heatmap of Record Counts by OTE_ID and LKE_ID')\n","plt.xlabel('LKE_ID')  # Setting the label for the x-axis\n","plt.ylabel('OTE_ID')  # Setting the label for the y-axis\n","\n","# Optimizing the layout for better viewing\n","plt.tight_layout()\n","\n","# Displaying the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vJG_bVh7NPz"},"outputs":[],"source":["# Group the data by 'OTE_ID' and calculate the percentage of missing values for each group\n","missing_percentage_by_ote_id = (party_merged.groupby('OTE_ID')['LKE_ID']\n","                                .apply(lambda x: (x.isna().sum() / len(x)) * 100)\n","                                .rename('Percentage Missing')\n","                                .reset_index())\n","\n","# Display the result\n","print(missing_percentage_by_ote_id)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RNdivrj37NP0"},"outputs":[],"source":["# Count VKL_NUMMER entries before removal\n","total_entries = len(party_merged['VKL_NUMMER'].unique())\n","severe_0_entries_before = len(party_merged[(party_merged['SEVERE'] == 0)]['VKL_NUMMER'].unique())\n","severe_1_entries_before = len(party_merged[(party_merged['SEVERE'] == 1)]['VKL_NUMMER'].unique())\n","\n","# Remove VKL_NUMMER with 100% NaN values in either OTE_ID or LKE_ID\n","nan_vkl_numbers = nan_percentages[nan_percentages['OTE_ID_NaN_Percentage'] == 100]['VKL_NUMMER'].unique()\n","nan_vkl_numbers = set(nan_vkl_numbers).union(set(nan_percentages[nan_percentages['LKE_ID_NaN_Percentage'] == 100]['VKL_NUMMER'].unique()))\n","party_merged = party_merged[~party_merged['VKL_NUMMER'].isin(nan_vkl_numbers)]\n","\n","# Count VKL_NUMMER entries after removal\n","total_entries_after = len(party_merged['VKL_NUMMER'].unique())\n","severe_0_entries_after = len(party_merged[(party_merged['SEVERE'] == 0)]['VKL_NUMMER'].unique())\n","severe_1_entries_after = len(party_merged[(party_merged['SEVERE'] == 1)]['VKL_NUMMER'].unique())\n","\n","print(f\"Total VKL_NUMMER entries deleted: {total_entries - total_entries_after}\")\n","print(f\"Deleted VKL_NUMMER entries with SEVERE=0: {severe_0_entries_before - severe_0_entries_after}\")\n","print(f\"Deleted VKL_NUMMER entries with SEVERE=1: {severe_1_entries_before - severe_1_entries_after}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDo8kwPd7NP0"},"outputs":[],"source":["party_merged_filtered=party_merged"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAWzIZEQ7NP0"},"outputs":[],"source":["# Save the DataFrame to a CSV file\n","party_merged_filtered.to_csv('Processed data/party_merged_filtered.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"SNEW_X537NP0"},"source":["## Checkpoint!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5-IhANl7NP0"},"outputs":[],"source":["party = pd.read_csv('Processed data/party_merged_filtered.csv')\n","objecttypes_grouped= pd.read_csv('Data raw/ReferentiebestandenOngevallen/ObjecttypesV2.csv', delimiter=';')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bg6gHc0e7NP0"},"outputs":[],"source":["party['OTE_ID']=party['OTE_ID'].astype('float64')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1mldVfl7NP0"},"outputs":[],"source":["party_obj=pd.merge(party,objecttypes_grouped,on='OTE_ID')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"psh8Kpba7NP0"},"outputs":[],"source":["party_obj.drop(columns=['OTE_OMS','OTE_ID'],axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AL1AS7Sp7NP0"},"outputs":[],"source":["import pandas as pd\n","\n","value_counts_OTE_ID = party_obj['Group'].value_counts(dropna=False)\n","value_counts_RIJBEWGEL = party_obj['RIJBEWGEL'].value_counts(dropna=False)\n","value_counts_RIJBEWBEG = party_obj['RIJBEWBEG'].value_counts(dropna=False)\n","value_counts_TDT_ID_1 = party_obj['TDT_ID_1'].value_counts(dropna=False)\n","\n","print(\"Value counts for RIJBEWGEL, including NaNs:\")\n","print(value_counts_RIJBEWGEL)\n","print(\"\\nValue counts for RIJBEWBEG, including NaNs:\")\n","print(value_counts_RIJBEWBEG)\n","print(\"\\nValue counts for TDT_ID_1, including NaNs:\")\n","print(value_counts_TDT_ID_1)\n","\n","print(value_counts_OTE_ID)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hgVFourc7NP0"},"outputs":[],"source":["party_obj['PTJ_ID_count'] = party_obj.groupby('VKL_NUMMER').apply(\n","    lambda x: x.loc[x['Group'].notna() & (x['Group'] != 'Object'), 'PTJ_ID'].count()\n",").reset_index(level=0, drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QqsOVe57NP0"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 8))\n","party_obj['PTJ_ID_count'].value_counts().sort_index().plot(kind='bar', color='skyblue')\n","plt.title('Log-Scale Bar Plot of PTJ_ID_count')\n","plt.xlabel('PTJ_ID_count')\n","plt.ylabel('Frequency (log scale)')\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nBOFsWTS7NP0"},"outputs":[],"source":["import numpy as np\n","\n","# Calculate z-scores for PTJ_ID_count\n","z_scores = np.abs((party_obj['PTJ_ID_count'] - party_obj['PTJ_ID_count'].mean()) / party_obj['PTJ_ID_count'].std())\n","\n","# Find PTJ_ID_count values for which the z-score is above 3\n","outliers = party_obj[z_scores > 3]['PTJ_ID_count'].unique()\n","\n","# Count the number of 'SEVERE'=1 occurrences for outliers\n","severe_1_count_for_outliers = party_obj[(party_obj['SEVERE'] == 1) & party_obj['PTJ_ID_count'].isin(outliers)]['SEVERE'].sum()\n","\n","# Count the number of 'SEVERE'=1 occurrences for all data\n","total_severe_1_count = party_obj[party_obj['SEVERE'] == 1]['SEVERE'].sum()\n","\n","print(\"PTJ_ID_count values with z-score above 3:\", outliers)\n","print(\"Number of 'SEVERE'=1 occurrences that would be deleted if outliers are removed:\", severe_1_count_for_outliers)\n","print(\"Total number of 'SEVERE'=1 occurrences in the dataset:\", total_severe_1_count)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uRzg7zq-7NP0"},"outputs":[],"source":["import numpy as np\n","\n","# Calculate z-scores for PTJ_ID_count\n","z_scores = np.abs((party_obj['PTJ_ID_count'] - party_obj['PTJ_ID_count'].mean()) / party_obj['PTJ_ID_count'].std())\n","\n","# Find PTJ_ID_count values for which the z-score is above 3\n","outliers = party_obj[z_scores > 3]['PTJ_ID_count'].unique()\n","\n","# Group the data by VKL_NUMMER and count the number of 'SEVERE'=1 occurrences\n","severe_1_count_per_vkl_nummer = party_obj[party_obj['SEVERE'] == 1].groupby('VKL_NUMMER')['SEVERE'].sum()\n","\n","# Filter the counts for VKL_NUMMER values that are outliers\n","severe_1_count_for_outliers = severe_1_count_per_vkl_nummer[severe_1_count_per_vkl_nummer.index.isin(outliers)].sum()\n","\n","# Count the total number of 'SEVERE'=1 occurrences in the dataset\n","total_severe_1_count = party_obj[party_obj['SEVERE'] == 1]['SEVERE'].sum()\n","\n","print(\"Number of 'SEVERE'=1 occurrences that would be deleted if outliers are removed:\", severe_1_count_for_outliers)\n","print(\"Total number of 'SEVERE'=1 occurrences in the dataset:\", total_severe_1_count)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nShDmKOG7NP0"},"outputs":[],"source":["severity_distribution = party_obj.groupby('PTJ_ID_count')['SEVERE'].unique()\n","\n","print(severity_distribution)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXYVA2NE7NP0"},"outputs":[],"source":["# Calculate z-scores for PTJ_ID_count\n","z_scores = np.abs((party_obj['PTJ_ID_count'] - party_obj['PTJ_ID_count'].mean()) / party_obj['PTJ_ID_count'].std())\n","\n","# Find PTJ_ID_count values for which the z-score is above 3\n","outliers = party_obj[z_scores > 3]['PTJ_ID_count'].unique()\n","\n","# Filter the dataset to remove outliers\n","party_obj_no_outliers = party_obj[~party_obj['PTJ_ID_count'].isin(outliers)]\n","\n","# Count the number of 'SEVERE'=1 occurrences in the original dataset\n","total_severe_1_count_original = party_obj[party_obj['SEVERE'] == 1]['SEVERE'].sum()\n","\n","# Count the number of 'SEVERE'=1 occurrences in the dataset without outliers\n","total_severe_1_count_no_outliers = party_obj_no_outliers[party_obj_no_outliers['SEVERE'] == 1]['SEVERE'].sum()\n","\n","# Calculate the difference in 'SEVERE'=1 occurrences between the original dataset and the dataset without outliers\n","severe_1_deleted_with_outliers_removed = total_severe_1_count_original - total_severe_1_count_no_outliers\n","\n","print(\"Number of 'SEVERE'=1 occurrences that would be deleted if outliers are removed:\", severe_1_deleted_with_outliers_removed)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZQgymLy7NP1"},"outputs":[],"source":["# Filter the dataset to include only rows where SEVERE=1\n","severe_1_data = party_obj[party_obj['SEVERE'] == 1]\n","\n","# Count the number of unique VKL_NUMMER values with SEVERE=1 in the original dataset\n","unique_vkl_nummers_original = severe_1_data['VKL_NUMMER'].nunique()\n","\n","# Filter the dataset to remove outliers in PTJ_ID_count\n","party_obj_no_outliers = party_obj[~party_obj['PTJ_ID_count'].isin(outliers)]\n","\n","# Filter the dataset without outliers to include only rows where SEVERE=1\n","severe_1_data_no_outliers = party_obj_no_outliers[party_obj_no_outliers['SEVERE'] == 1]\n","\n","# Count the number of unique VKL_NUMMER values with SEVERE=1 in the dataset without outliers\n","unique_vkl_nummers_no_outliers = severe_1_data_no_outliers['VKL_NUMMER'].nunique()\n","\n","# Calculate the difference in unique VKL_NUMMER counts with SEVERE=1 between the original dataset and the dataset without outliers\n","lost_vkl_nummers_with_outliers_removed = unique_vkl_nummers_original - unique_vkl_nummers_no_outliers\n","\n","print(\"Number of VKL_NUMMERS with SEVERE=1 that would be lost if outliers are removed:\", lost_vkl_nummers_with_outliers_removed)\n","# Calculate the total number of rows represented by VKL_NUMMER values with SEVERE=1 in the original dataset\n","total_rows_severe_1_original = severe_1_data.shape[0]\n","\n","# Calculate the total number of unique VKL_NUMMER values with SEVERE=1 in the original dataset\n","unique_vkl_nummers_severe_1_original = severe_1_data['VKL_NUMMER'].nunique()\n","\n","# Calculate the total number of rows represented by VKL_NUMMER values with SEVERE=1 that would be lost if outliers are removed\n","total_rows_severe_1_no_outliers = severe_1_data_no_outliers.shape[0]\n","\n","# Calculate the total number of unique VKL_NUMMER values with SEVERE=1 that would be lost if outliers are removed\n","unique_vkl_nummers_severe_1_no_outliers = severe_1_data_no_outliers['VKL_NUMMER'].nunique()\n","\n","# Calculate the loss in VKL_NUMMER for which SEVERE=1 and the total rows of VKL_NUMMERS if outliers are removed\n","lost_vkl_nummers_severe_1 = unique_vkl_nummers_severe_1_original - unique_vkl_nummers_severe_1_no_outliers\n","lost_total_rows_severe_1 = total_rows_severe_1_original - total_rows_severe_1_no_outliers\n","\n","print(\"Loss in VKL_NUMMER for which SEVERE=1 if outliers are removed:\", lost_vkl_nummers_severe_1)\n","print(\"Loss in total rows of VKL_NUMMERS if outliers are removed:\", lost_total_rows_severe_1)\n","\n","\n","# Calculate the total number of rows with SEVERE=1 in the original dataset\n","total_severe_1_original = party_obj[party_obj['SEVERE'] == 1].shape[0]\n","\n","# Calculate the total number of unique VKL_NUMMER values with SEVERE=1 in the original dataset\n","unique_vkl_nummers_severe_1_original = severe_1_data['VKL_NUMMER'].nunique()\n","\n","# Calculate the percentage of SEVERE=1 occurrences that would be lost if outliers are removed\n","percentage_severe_1_lost = (lost_total_rows_severe_1 / total_severe_1_original) * 100\n","\n","# Calculate the percentage of unique VKL_NUMMER values with SEVERE=1 that would be lost if outliers are removed\n","percentage_vkl_nummers_severe_1_lost = (lost_vkl_nummers_severe_1 / unique_vkl_nummers_severe_1_original) * 100\n","\n","print(\"Percentage of SEVERE=1 occurrences lost if outliers are removed:\", percentage_severe_1_lost)\n","print(\"Percentage of unique VKL_NUMMER values with SEVERE=1 lost if outliers are removed:\", percentage_vkl_nummers_severe_1_lost)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DbWAfsic7NP2"},"outputs":[],"source":["# Calculate z-scores for PTJ_ID_count\n","z_scores = np.abs((party_obj['PTJ_ID_count'] - party_obj['PTJ_ID_count'].mean()) / party_obj['PTJ_ID_count'].std())\n","\n","# Find PTJ_ID_count values for which the z-score is above 3\n","outliers = party_obj[z_scores > 3]['PTJ_ID_count'].unique()\n","\n","# Count the number of VKL_NUMMERS and 'SEVERE'=1 for each frequency of PTJ_ID_count values\n","counts_per_frequency = party_obj.groupby('PTJ_ID_count').agg({'VKL_NUMMER': 'count', 'SEVERE': lambda x: (x == 1).sum()}).reset_index()\n","counts_per_frequency.rename(columns={'VKL_NUMMER': 'VKL_NUMMER_Count', 'SEVERE': 'SEVERE=1_Count'}, inplace=True)\n","\n","# Calculate percentage of 'SEVERE'=1 occurrences\n","counts_per_frequency['SEVERE=1_Percentage'] = (counts_per_frequency['SEVERE=1_Count'] / counts_per_frequency['VKL_NUMMER_Count']) * 100\n","\n","print(\"PTJ_ID_count values with z-score above 3:\", outliers)\n","print(\"Number of VKL_NUMMERS, 'SEVERE'=1 count, and percentage of 'SEVERE'=1 per frequency of PTJ_ID_count values:\")\n","print(counts_per_frequency)\n"]},{"cell_type":"markdown","metadata":{"id":"oZERfhEM7NP2"},"source":["Niet verwijderen wordt toch geaggregeert naar ratios gezien dat absolute aantallen information leakage kunnen zijn"]},{"cell_type":"markdown","metadata":{"id":"4stdWHY47NP2"},"source":["New zeeland paper had de dezelfde verdeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDAPK-DR7NP2"},"outputs":[],"source":["party_obj['Party_cat'] = party_obj['PTJ_ID_count'].apply(lambda x: 'single-party' if x == 1 else ('two-party' if x == 2 else 'multiparty'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjV5yI1Z7NP2"},"outputs":[],"source":["party_obj.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBnaPLHN7NP2"},"outputs":[],"source":["# Set display options to show all columns and rows\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qs9UgqJj7NP2"},"outputs":[],"source":["party_obj['BWG_ID_1'].value_counts(dropna=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGhoFB0L7NP2"},"outputs":[],"source":["party_obj['VTGVERZ'].value_counts(dropna=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9e15uLG7NP2"},"outputs":[],"source":["#Only yes\n","party_obj['RIJBEWGEL'].value_counts(dropna=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMkjMo_I7NP2"},"outputs":[],"source":["#Could be interesting but so many empty values i think it is not worth it (better to use different dataset for this type of research)\n","party_obj['AGT']= party_obj['AGT_ID_1'].map({11: 'Voor', 12: 'Voor', 13: 'Voor', 14: 'Achter', 15: 'Achter', 16: 'Achter', 17: 'Achter', 18: 'Flank'})\n","party_obj['AGT'].value_counts(dropna=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yWmzsbuX7NP2"},"outputs":[],"source":["#Only yes for 3k values, not the dataset to research this\n","party_obj['RIJBEWBEG'].value_counts(dropna=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqgxEAv47NP2"},"outputs":[],"source":["#unfortunately to empty to use, to research this aswell better to use a different set\n","party_obj['TDT_ID_1'].value_counts(dropna=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPRxWn-B7NP2"},"outputs":[],"source":["# Function to calculate gender ratio\n","def gender_ratio(series):\n","    counts = series.value_counts(normalize=True)\n","    return counts.to_dict()\n","\n","# Custom function to safely extract the mode\n","def safe_mode(series):\n","    mode_values = series.mode()\n","    if not mode_values.empty:\n","        return mode_values[0]  # Return the first mode value if exists\n","    else:\n","        return None  # Return None if the mode cannot be determined\n","\n","# Aggregation functions\n","aggregations = {\n","    'GESLACHT': gender_ratio,\n","    'Party_cat': 'first',\n","    'LKE_ID': ['min', 'max', safe_mode],\n","    'Group': lambda x: ', '.join(x.dropna().astype(str).unique())\n","}\n","\n","# Group by 'VKL_NUMMER' and apply aggregation\n","grouped = party_obj.groupby('VKL_NUMMER').agg(aggregations)\n","\n","# Flatten the column names\n","grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n","\n","# Rename columns to make them more readable\n","grouped.rename(columns={\n","    'GESLACHT_gender_ratio': 'Gender Ratio',\n","    'Party_cat_first': 'Party Category',\n","    'LKE_ID_min': 'Min Age Group',\n","    'LKE_ID_max': 'Max Age Group',\n","    'LKE_ID_safe_mode': 'Mode Age Group',  # Correctly name the mode column\n","    'Group_<lambda>': 'Group'\n","}, inplace=True)\n","\n","print(grouped)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_l51AKaq7NP3"},"outputs":[],"source":["pd.set_option('display.max_rows', None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GfPDYwFk7NP3"},"outputs":[],"source":["grouped.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b075pq6w7NP3"},"outputs":[],"source":["grouped.reset_index(inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tyahEWXO7NP3"},"outputs":[],"source":["grouped.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_VoiGm3g7NP3"},"outputs":[],"source":["grouped.to_csv('Processed data/party_grouped.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"O0TfL2um7NP3"},"source":["## Checkpoint!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZHeMz0V7NP3"},"outputs":[],"source":["grouped= pd.read_csv('Processed data/party_grouped.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8RHu2Bfs7NP3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eojRvQmf7NP3"},"outputs":[],"source":["import squarify\n","import seaborn as sb\n","\n","# Combine 'Party Category' and 'Group', and standardize/sort groups lexicographically\n","grouped['Combined Group'] = grouped['Party Category'] + \": \" + grouped['Group'].apply(lambda x: ', '.join(sorted(x.split(', '))))\n","\n","# Count the occurrences of each sorted combined group\n","combined_group_counts = grouped['Combined Group'].value_counts()\n","\n","# Calculate the 2% threshold of the total counts\n","threshold = combined_group_counts.sum() * 0.01\n","\n","# Filter out categories below the threshold\n","small_groups = combined_group_counts[combined_group_counts < threshold]\n","other_sum = small_groups.sum()  # Sum of all small group counts\n","combined_group_counts = combined_group_counts[combined_group_counts >= threshold]  # Keep only groups above threshold\n","\n","# Add 'Other' category if there are any small groups\n","if other_sum > 0:\n","    combined_group_counts['Other'] = other_sum\n","\n","# Prepare data for plotting\n","labels = [f\"{label}\\n({value / 1000:.1f}K)\" for label, value in zip(combined_group_counts.index, combined_group_counts.values)]\n","sizes = combined_group_counts.values\n","colors = plt.cm.Spectral((sizes - sizes.min()) / (sizes.max() - sizes.min()))\n","\n","# Plot\n","plt.figure(figsize=(12, 8))\n","squarify.plot(sizes=sizes, label=labels, color=sb.color_palette(\"rocket\", len(labels)), text_kwargs={'fontsize': 10, 'color': 'white'}, alpha=0.8, pad=0.25)\n","plt.title('Treemap of Combined Group Value Counts with \"Other\" Group for the Lowest 2%')\n","plt.axis('off')  # Turn off axis\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xm5OSPaK7NP3"},"outputs":[],"source":["# Combine 'Party Category' and 'Group', and standardize/sort groups lexicographically\n","grouped['Combined Group'] = grouped['Party Category'] + \": \" + grouped['Group'].apply(lambda x: ', '.join(sorted(x.split(', '))))\n","\n","# Count the occurrences of each sorted combined group\n","combined_group_counts = grouped['Combined Group'].value_counts()\n","\n","# Calculate the 2% threshold of the total counts\n","threshold = combined_group_counts.sum() * 0.01\n","\n","# Filter out categories below the threshold\n","small_groups = combined_group_counts[combined_group_counts < threshold]\n","other_sum = small_groups.sum()  # Sum of all small group counts\n","combined_group_counts = combined_group_counts[combined_group_counts >= threshold]  # Keep only groups above threshold\n","\n","# Add 'Other' category if there are any small groups\n","if other_sum > 0:\n","    combined_group_counts['Other'] = other_sum\n","\n","# Prepare data for the table\n","table_data = combined_group_counts.reset_index()\n","table_data.columns = ['Combined Group', 'Counts']\n","\n","# Print the table\n","print(table_data.to_string(index=False))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zzwC37g7NP3"},"outputs":[],"source":["grouped = grouped.merge(rta[['VKL_NUMMER', 'SEVERE']], on='VKL_NUMMER', how='left')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-c3tV3-27NP3"},"outputs":[],"source":["grouped['SEVERE'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_t-Q-RAm7NP3"},"outputs":[],"source":["rta['SEVERE'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WF8S0ul77NP3"},"outputs":[],"source":["pa=rta.merge(grouped,on='VKL_NUMMER', how='inner')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkhNTdso7NP3"},"outputs":[],"source":["pa.to_csv('Processed data/pa.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWSNbVE67NP3"},"outputs":[],"source":["pa= pd.read_csv('Processed data/pa.csv')"]},{"cell_type":"markdown","metadata":{"id":"r1SE_QHE7NP3"},"source":["## EDA before and after"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7S7Bq187NP3"},"outputs":[],"source":["\n","# Function to compute proportion of severe cases\n","def proportion_severe(values):\n","    return np.mean(values)  # Average of 1s and 0s gives us the proportion of 1s\n","\n","# Set up the figure and axes\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), sharey=True)\n","\n","# Hexbin for data density\n","hb_density = ax1.hexbin(rta['longitude'], rta['latitude'], gridsize=7, cmap='Reds',\n","                        bins='log', mincnt=1)  # Logarithmic count of points per hexbin\n","ax1.set_title('Data Density (Log Scale)')\n","ax1.set_xlabel('Longitude')\n","ax1.set_ylabel('Latitude')\n","fig.colorbar(hb_density, ax=ax1, label='Logarithmic scale of data density')\n","\n","# Hexbin for proportion of severe cases\n","hb_severe = ax2.hexbin(rta['longitude'], rta['latitude'], C=rta['SEVERE'],\n","                       gridsize=7, reduce_C_function=proportion_severe, cmap='Reds',\n","                       norm=mcolors.LogNorm(vmin=0.01, vmax=1))  # Logarithmic normalization\n","ax2.set_title('Proportion of SEVERE Cases')\n","ax2.set_xlabel('Longitude')\n","fig.colorbar(hb_severe, ax=ax2, label='Logarithmic scale of proportion of SEVERE cases')\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6M1I_RF7NP3"},"outputs":[],"source":["# Create bin centers and spatial index from 'rta' using hexbin\n","fig, ax = plt.subplots()\n","hb_rta = ax.hexbin(rta['longitude'], rta['latitude'], gridsize=6, cmap='coolwarm', reduce_C_function=np.mean)\n","plt.close(fig)  # No need to display this figure\n","\n","# Retrieve bin centers\n","bin_centers = hb_rta.get_offsets()\n","\n","# Build spatial index using cKDTree\n","tree = cKDTree(bin_centers)\n","\n","# Find the nearest hexbin index for each data point in RTA\n","distances, indices = tree.query(rta[['longitude', 'latitude']])\n","\n","# Assign bin index to each entry in 'rta'\n","rta['bin_index'] = indices\n","\n","# Sort bins by latitude to order them from north to south\n","sorted_indices = np.argsort(-bin_centers[:, 1])\n","new_ids = np.argsort(sorted_indices)\n","\n","# Remap bin indices in 'rta' using sorted indices\n","rta['sorted_bin_index'] = new_ids[rta['bin_index']]\n","\n","print(rta.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x8AFh-zt7NP4"},"outputs":[],"source":["# Create a hexbin plot with Matplotlib and apply logarithmic normalization\n","fig, ax = plt.subplots()\n","hb = ax.hexbin(rta['longitude'], rta['latitude'], gridsize=6, cmap='OrRd',\n","               reduce_C_function=np.mean, norm=LogNorm())\n","ax.set_title('Hexagonal Binning of Data Points (Log Scale)')\n","ax.set_xlabel('Longitude')\n","ax.set_ylabel('Latitude')\n","cb = plt.colorbar(hb)\n","cb.set_label('Logarithmic scale of counts in bin')\n","\n","# Retrieve bin centers and counts\n","bin_centers = hb.get_offsets()\n","counts = hb.get_array()\n","\n","# Filter out empty bins (where count is zero)\n","non_empty_bins = counts > 0\n","filtered_bin_centers = bin_centers[non_empty_bins]\n","\n","# Build a spatial index using cKDTree for non-empty bins\n","tree = cKDTree(filtered_bin_centers)\n","\n","# Find the nearest hexbin for each data point among non-empty bins\n","distances, indices = tree.query(rta[['longitude', 'latitude']].values)\n","\n","# Save the bin index and bin center coordinates to the DataFrame\n","rta['hexbin_id'] = indices\n","rta['hexbin_center_longitude'] = filtered_bin_centers[indices, 0]\n","rta['hexbin_center_latitude'] = filtered_bin_centers[indices, 1]\n","\n","# Sort bins by latitude to order them from north to south\n","sorted_indices = np.argsort(-filtered_bin_centers[:, 1])  # Sort by negative latitude for descending order\n","new_ids = np.argsort(sorted_indices)  # This gives a new order to the indices\n","rta['sorted_hexbin_id'] = new_ids[rta['hexbin_id']]\n","\n","# Annotate the plot with sorted bin IDs\n","for i, center in enumerate(filtered_bin_centers):\n","    sorted_id = new_ids[i]\n","    ax.text(center[0], center[1], str(sorted_id), color='blue', ha='center', va='center')\n","\n","plt.legend(['Bin Centers'])\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GGUGGDvF7NP4"},"outputs":[],"source":["# Extracting hexbin_id from rta\n","hexbin_data = rta[['VKL_NUMMER', 'sorted_hexbin_id','hexbin_center_longitude','hexbin_center_latitude']]\n","\n","# Merging hexbin_id with pa\n","pa = pd.merge(pa, hexbin_data, on='VKL_NUMMER', how='left')\n","\n","# Check for any NaN values in the merged DataFrame\n","missing_values = pa['sorted_hexbin_id'].isna().sum()\n","if missing_values > 0:\n","    print(f\"There are {missing_values} rows in 'pa' with no corresponding 'hexbin_id'.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RI6g5ilY7NP4"},"outputs":[],"source":["from scipy.spatial import cKDTree\n","\n","# Function to create hexbin plot with annotations\n","def create_hexbin_plot(data, title):\n","    fig, ax = plt.subplots(figsize=(10,8))\n","    hb = ax.hexbin(data['longitude'], data['latitude'], gridsize=6, cmap='coolwarm',\n","                   reduce_C_function=np.mean, norm=LogNorm())\n","    ax.set_title(title)\n","    ax.set_xlabel('Longitude')\n","    ax.set_ylabel('Latitude')\n","    cb = plt.colorbar(hb)\n","    cb.set_label('Logarithmic scale of counts in bin')\n","\n","    # Retrieve bin centers and counts\n","    bin_centers = hb.get_offsets()\n","    counts = hb.get_array()\n","\n","    # Filter out empty bins (where count is zero)\n","    non_empty_bins = counts > 0\n","    filtered_bin_centers = bin_centers[non_empty_bins]\n","\n","    # Build a spatial index using cKDTree for non-empty bins\n","    tree = cKDTree(filtered_bin_centers)\n","\n","    # Find the nearest hexbin for each data point among non-empty bins\n","    distances, indices = tree.query(data[['longitude', 'latitude']].values)\n","\n","    # Save the bin index and bin center coordinates to the DataFrame\n","    data['hexbin_id'] = indices\n","    data['hexbin_center_longitude'] = filtered_bin_centers[indices, 0]\n","    data['hexbin_center_latitude'] = filtered_bin_centers[indices, 1]\n","\n","    # Sort bins by latitude to order them from north to south\n","    sorted_indices = np.argsort(-filtered_bin_centers[:, 1])  # Sort by negative latitude for descending order\n","    new_ids = np.argsort(sorted_indices)  # This gives a new order to the indices\n","    data['sorted_hexbin_id'] = new_ids[data['hexbin_id']]\n","\n","    # Annotate the plot with sorted bin IDs\n","    for i, center in enumerate(filtered_bin_centers):\n","        sorted_id = new_ids[i]\n","        ax.text(center[0], center[1], str(sorted_id), color='blue', ha='center', va='center')\n","\n","    plt.legend(['Bin Centers'])\n","    plt.show()\n","\n","# Create hexbin plot with annotations for rta\n","create_hexbin_plot(rta, 'Hexagonal Binning of Data Points for rta (Log Scale)')\n","\n","# Create hexbin plot with annotations for pa\n","create_hexbin_plot(pa, 'Hexagonal Binning of Data Points for pa (Log Scale)')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"45Mbzo1G7NP4"},"outputs":[],"source":["pa.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzU7oFv17NP4"},"outputs":[],"source":["pa['SEVERE']=pa['SEVERE_x']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsEu9M6_7NP4"},"outputs":[],"source":["pa['sorted_hexbin_id'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7T3JnwR67NP4"},"outputs":[],"source":["# Function to calculate the ratio of SEVERE=1 to total counts in each bin\n","def ratio_of_severe(x):\n","    return np.sum(x) / len(x)\n","\n","# Modified function to create hexbin plot with custom aggregation\n","def create_hexbin_plot(data, title):\n","    fig, ax = plt.subplots()\n","    hb = ax.hexbin(data['longitude'], data['latitude'], C=data['SEVERE'],\n","                   gridsize=6, cmap='coolwarm', reduce_C_function=ratio_of_severe, norm=LogNorm())\n","    ax.set_title(title)\n","    ax.set_xlabel('Longitude')\n","    ax.set_ylabel('Latitude')\n","    cb = plt.colorbar(hb)\n","    cb.set_label('Ratio of SEVERE=1')\n","\n","    # Retrieve bin centers and counts\n","    bin_centers = hb.get_offsets()\n","    counts = hb.get_array()\n","\n","    # Filter out empty bins (where count is zero)\n","    non_empty_bins = counts > 0\n","    filtered_bin_centers = bin_centers[non_empty_bins]\n","\n","    # Build a spatial index using cKDTree for non-empty bins\n","    tree = cKDTree(filtered_bin_centers)\n","\n","    # Find the nearest hexbin for each data point among non-empty bins\n","    distances, indices = tree.query(data[['longitude', 'latitude']].values)\n","\n","    # Save the bin index and bin center coordinates to the DataFrame\n","    data['hexbin_id'] = indices\n","    data['hexbin_center_longitude'] = filtered_bin_centers[indices, 0]\n","    data['hexbin_center_latitude'] = filtered_bin_centers[indices, 1]\n","\n","    # Sort bins by latitude to order them from north to south\n","    sorted_indices = np.argsort(-filtered_bin_centers[:, 1])  # Sort by negative latitude for descending order\n","    new_ids = np.argsort(sorted_indices)  # This gives a new order to the indices\n","    data['sorted_hexbin_id'] = new_ids[data['hexbin_id']]\n","\n","    # Annotate the plot with sorted bin IDs\n","    for i, center in enumerate(filtered_bin_centers):\n","        sorted_id = new_ids[i]\n","        ax.text(center[0], center[1], str(sorted_id), color='blue', ha='center', va='center')\n","\n","    plt.legend(['Bin Centers'])\n","    plt.show()\n","\n","# Create plots\n","create_hexbin_plot(rta, 'Hexagonal Binning of SEVERE Ratio for rta (Log Scale)')\n","create_hexbin_plot(pa, 'Hexagonal Binning of SEVERE Ratio for pa (Log Scale)')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CRIvgq6X7NP4"},"outputs":[],"source":["pa.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IS1Wlgbc7NP4"},"outputs":[],"source":["pa.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlXM0hAg7NP4"},"outputs":[],"source":["rta.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPwLMIXL7NP4"},"outputs":[],"source":["pa.to_csv('Processed data/pa.csv',index=False)"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}